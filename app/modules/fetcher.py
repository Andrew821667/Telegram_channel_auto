"""
News Fetcher Module
–õ–µ–≥–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
1. Google News RSS (—Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
2. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
3. Telegram –∫–∞–Ω–∞–ª—ã (—Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç)
"""

import asyncio
import random
from datetime import datetime
from typing import List, Optional, Dict, Any
from urllib.parse import urlencode, quote_plus

import feedparser
import httpx
from bs4 import BeautifulSoup
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.config import settings
from app.models.database import RawArticle, Source, log_to_db
import structlog

logger = structlog.get_logger()


# User-Agent —Ä–æ—Ç–∞—Ü–∏—è –¥–ª—è –ª–µ–≥–∞–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
]


class NewsFetcher:
    """–°–±–æ—Ä—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""

    def __init__(self, db_session: AsyncSession):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fetcher.

        Args:
            db_session: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        """
        self.db = db_session
        self.client: Optional[httpx.AsyncClient] = None

    async def __aenter__(self):
        """Async context manager entry."""
        self.client = httpx.AsyncClient(
            timeout=settings.fetcher_request_timeout,
            follow_redirects=True,
            headers={"User-Agent": self._get_random_user_agent()}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.client:
            await self.client.aclose()

    def _get_random_user_agent(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏."""
        return random.choice(USER_AGENTS)

    async def _fetch_with_retry(
        self,
        url: str,
        max_retries: Optional[int] = None
    ) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º.

        Args:
            url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫

        Returns:
            –ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if max_retries is None:
            max_retries = settings.fetcher_max_retries

        for attempt in range(max_retries):
            try:
                # Rate limiting - 1 –∑–∞–ø—Ä–æ—Å –≤ —Å–µ–∫—É–Ω–¥—É
                if attempt > 0:
                    delay = settings.fetcher_retry_delay * (2 ** attempt)  # Exponential backoff
                    await asyncio.sleep(delay)
                else:
                    await asyncio.sleep(1)  # Base rate limit

                # –û–±–Ω–æ–≤–ª—è–µ–º User-Agent –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
                self.client.headers["User-Agent"] = self._get_random_user_agent()

                response = await self.client.get(url)
                response.raise_for_status()

                logger.info(
                    "fetch_success",
                    url=url,
                    status_code=response.status_code,
                    attempt=attempt + 1
                )

                return response.text

            except httpx.HTTPError as e:
                logger.warning(
                    "fetch_error",
                    url=url,
                    error=str(e),
                    attempt=attempt + 1,
                    max_retries=max_retries
                )

                if attempt == max_retries - 1:
                    logger.error(
                        "fetch_failed",
                        url=url,
                        error=str(e),
                        total_attempts=max_retries
                    )
                    await log_to_db(
                        "ERROR",
                        f"Failed to fetch URL after {max_retries} attempts",
                        {"url": url, "error": str(e)},
                        session=self.db  # –ü–µ—Ä–µ–¥–∞—ë–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–µ—Å—Å–∏—é
                    )
                    return None

        return None

    def _build_google_news_rss_url(
        self,
        query: str,
        lang: str = "ru",
        region: str = "RU"
    ) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL –¥–ª—è Google News RSS.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            lang: –Ø–∑—ã–∫ (ru, en)
            region: –†–µ–≥–∏–æ–Ω (RU, US)

        Returns:
            URL –¥–ª—è RSS feed
        """
        params = {
            "q": query,
            "hl": lang,
            "gl": region,
            "ceid": f"{region}:{lang}"
        }
        return f"{settings.google_news_rss_url}?{urlencode(params, quote_via=quote_plus)}"

    async def fetch_google_news_rss(self, lang: str = "ru") -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Google News RSS.

        Args:
            lang: –Ø–∑—ã–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (ru –∏–ª–∏ en)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∏ —Ä–µ–≥–∏–æ–Ω –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        if lang == "ru":
            query = settings.google_news_query_ru
            region = settings.google_news_region
        else:
            query = settings.google_news_query_en
            region = "US"

        rss_url = self._build_google_news_rss_url(query, lang, region)

        logger.info("fetching_google_news", lang=lang, url=rss_url)

        # –ü–æ–ª—É—á–∞–µ–º RSS feed
        content = await self._fetch_with_retry(rss_url)
        if not content:
            return articles

        # –ü–∞—Ä—Å–∏–º RSS
        feed = feedparser.parse(content)

        for entry in feed.entries[:settings.fetcher_max_articles_per_source]:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS entry
                article_data = {
                    "url": entry.link,
                    "title": entry.title,
                    "content": entry.get("summary", ""),
                    "source_name": f"Google News RSS ({lang.upper()})",
                    "published_at": self._parse_date(entry.get("published")),
                }

                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                full_content = await self._fetch_article_content(entry.link)
                if full_content:
                    article_data["content"] = full_content

                articles.append(article_data)

                logger.info(
                    "article_fetched",
                    source="google_news",
                    lang=lang,
                    title=article_data["title"][:50]
                )

            except Exception as e:
                logger.error(
                    "article_parse_error",
                    error=str(e),
                    entry_title=entry.get("title", "Unknown")
                )
                continue

        logger.info(
            "google_news_fetch_complete",
            lang=lang,
            articles_count=len(articles)
        )

        return articles

    async def _fetch_article_content(self, url: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

        Args:
            url: URL —Å—Ç–∞—Ç—å–∏

        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None
        """
        try:
            content = await self._fetch_with_retry(url)
            if not content:
                return None

            # –ü–∞—Ä—Å–∏–º HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
            soup = BeautifulSoup(content, "html.parser")

            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()

            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            # –ò—â–µ–º –ø–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ç–µ–≥–∞–º –¥–ª—è —Å—Ç–∞—Ç–µ–π
            article_tags = [
                soup.find("article"),
                soup.find("div", class_=lambda x: x and "content" in x.lower()),
                soup.find("div", class_=lambda x: x and "article" in x.lower()),
                soup.find("main"),
            ]

            for tag in article_tags:
                if tag:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                    text = tag.get_text(separator="\n", strip=True)
                    # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
                    text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
                    if len(text) > 200:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        return text[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ–≥–∏, –±–µ—Ä–µ–º –≤–µ—Å—å body
            body = soup.find("body")
            if body:
                text = body.get_text(separator="\n", strip=True)
                text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
                return text[:5000]

        except Exception as e:
            logger.warning(
                "content_fetch_error",
                url=url,
                error=str(e)
            )

        return None

    def _is_relevant_article(self, title: str, content: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å—Ç–∞—Ç—å–∏ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º AI + legal.

        Args:
            title: –ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏
            content: –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ —Å—Ç–∞—Ç—å–∏

        Returns:
            True –µ—Å–ª–∏ —Å—Ç–∞—Ç—å—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ —Ç–µ–º–µ AI + —é—Ä–∏—Å–ø—Ä—É–¥–µ–Ω—Ü–∏—è/–±–∏–∑–Ω–µ—Å
        """
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º title –∏ content –¥–ª—è –ø–æ–∏—Å–∫–∞
        text = f"{title} {content}".lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ AI (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ)
        ai_keywords = [
            # –†—É—Å—Å–∫–∏–µ
            "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç", "–∏–∏", "–Ω–µ–π—Ä–æ—Å–µ—Ç", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ",
            "chatgpt", "gpt", "openai", "claude", "gemini", "llm", "–Ω–µ–π—Ä–æ–Ω–Ω",
            "–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü", "—Ä–æ–±–æ—Ç–∏–∑–∞—Ü", "ml ", "ai ", "deep learning",
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ
            "artificial intelligence", "machine learning", "neural network",
            "automation", "robotics", "nlp", "computer vision"
        ]

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ legal/business (—Ä—É—Å—Å–∫–∏–µ –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–µ)
        legal_business_keywords = [
            # –†—É—Å—Å–∫–∏–µ - —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ
            "–ø—Ä–∞–≤–æ", "—Å—É–¥", "—é—Ä–∏—Å—Ç", "–∑–∞–∫–æ–Ω", "–¥–æ–≥–æ–≤–æ—Ä", "–ø—Ä–∞–≤–æ–≤", "—é—Ä–∏–¥–∏—á–µ—Å–∫",
            "compliance", "–∫–æ–º–ø–ª–∞–µ–Ω—Å", "—Ä–µ–≥—É–ª–∏—Ä–æ–≤", "–Ω–æ—Ä–º–∞—Ç–∏–≤–Ω", "—Å—É–¥–µ–±–Ω",
            # –†—É—Å—Å–∫–∏–µ - –±–∏–∑–Ω–µ—Å
            "–±–∏–∑–Ω–µ—Å", "–∫–æ–º–ø–∞–Ω", "–∫–æ—Ä–ø–æ—Ä–∞—Ç", "—É–ø—Ä–∞–≤–ª–µ–Ω", "—Ä–∏—Å–∫", "–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç",
            "–¥–∞–Ω–Ω—ã—Ö", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω", "gdpr", "–∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω",
            # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ
            "legal", "law", "court", "lawyer", "attorney", "contract",
            "regulation", "legaltech", "business", "corporate", "governance",
            "compliance", "risk", "data protection", "privacy"
        ]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ AI keyword
        has_ai = any(keyword in text for keyword in ai_keywords)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ–≥–æ legal/business keyword
        has_legal_or_business = any(keyword in text for keyword in legal_business_keywords)

        # –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞ –µ—Å–ª–∏ –µ—Å—Ç—å –ò ai –ò (legal –ò–õ–ò business)
        is_relevant = has_ai and has_legal_or_business

        if not is_relevant:
            logger.debug(
                "article_filtered_irrelevant",
                title=title[:100],
                has_ai=has_ai,
                has_legal_or_business=has_legal_or_business
            )

        return is_relevant

    async def fetch_rss_feed(self, source: Source) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ AI + legal/business.

        Args:
            source: –û–±—ä–µ–∫—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ –ë–î

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []
        filtered_count = 0

        logger.info("fetching_rss", source_name=source.name, url=source.url)

        content = await self._fetch_with_retry(source.url)
        if not content:
            return articles

        feed = feedparser.parse(content)

        for entry in feed.entries[:settings.fetcher_max_articles_per_source]:
            try:
                article_data = {
                    "url": entry.link,
                    "title": entry.title,
                    "content": entry.get("summary", ""),
                    "source_name": source.name,
                    "published_at": self._parse_date(entry.get("published")),
                }

                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                full_content = await self._fetch_article_content(entry.link)
                if full_content:
                    article_data["content"] = full_content

                # üîç –§–ò–õ–¨–¢–†–ê–¶–ò–Ø: –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ AI + legal/business
                if not self._is_relevant_article(article_data["title"], article_data["content"]):
                    filtered_count += 1
                    logger.info(
                        "article_filtered",
                        source=source.name,
                        title=article_data["title"][:80],
                        reason="not_ai_legal_business"
                    )
                    continue

                articles.append(article_data)

            except Exception as e:
                logger.error(
                    "rss_parse_error",
                    source=source.name,
                    error=str(e)
                )
                continue

        logger.info(
            "rss_fetch_complete",
            source_name=source.name,
            total_entries=len(feed.entries[:settings.fetcher_max_articles_per_source]),
            filtered_out=filtered_count,
            articles_accepted=len(articles)
        )

        return articles

    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.

        Args:
            date_str: –°—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π

        Returns:
            datetime –æ–±—ä–µ–∫—Ç –∏–ª–∏ None (–±–µ–∑ timezone)
        """
        if not date_str:
            return None

        try:
            # feedparser –æ–±—ã—á–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç parsed –≤—Ä–µ–º—è
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(date_str)
            # –£–±–∏—Ä–∞–µ–º timezone –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
            return dt.replace(tzinfo=None) if dt else None
        except Exception:
            try:
                # Fallback –Ω–∞ ISO —Ñ–æ—Ä–º–∞—Ç
                from dateutil import parser
                dt = parser.parse(date_str)
                # –£–±–∏—Ä–∞–µ–º timezone –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
                return dt.replace(tzinfo=None) if dt else None
            except Exception:
                logger.warning("date_parse_error", date_str=date_str)
                return None

    async def fetch_perplexity_news(self, lang: str = "ru") -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Perplexity AI real-time search.

        Args:
            lang: –Ø–∑—ã–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (ru –∏–ª–∏ en)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        if lang == "ru":
            query = settings.google_news_query_ru.replace(" AND ", " ")
            search_prompt = f"""–ù–∞–π–¥–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞) –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç:
- title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
- content: –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- url: —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
- source_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
- published_at: –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ISO 8601

–ò—â–∏ —Ç–æ–ª—å–∫–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏. –í–µ—Ä–Ω–∏ –º–∞–∫—Å–∏–º—É–º 10 –Ω–æ–≤–æ—Å—Ç–µ–π."""
        else:
            query = settings.google_news_query_en.replace(" AND ", " ")
            search_prompt = f"""Find latest news (from last 24 hours) for query: {query}

Return results as JSON array where each element contains:
- title: news headline
- content: brief summary (2-3 sentences)
- url: source link
- source_name: source name
- published_at: publication date in ISO 8601 format

Search only for recent news. Return maximum 10 articles."""

        logger.info("fetching_perplexity_news", lang=lang)

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM provider –¥–ª—è Perplexity
            from app.modules.llm_provider import get_llm_provider

            llm = get_llm_provider("perplexity")

            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ Perplexity —Å real-time search
            response = await llm.generate_completion(
                messages=[
                    {"role": "system", "content": "You are a news aggregator assistant. Always return valid JSON."},
                    {"role": "user", "content": search_prompt}
                ],
                max_tokens=3000,
                temperature=0.3
            )

            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            import json
            import re

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ markdown)
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON
                json_str = response.strip()

            try:
                news_data = json.loads(json_str)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                if not isinstance(news_data, list):
                    logger.warning("perplexity_response_not_list", response=response[:200])
                    return articles

                for item in news_data:
                    try:
                        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –µ—Å–ª–∏ –µ—Å—Ç—å
                        published_at = None
                        if "published_at" in item and item["published_at"]:
                            published_at = self._parse_date(item["published_at"])

                        # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é
                        article_data = {
                            "url": item.get("url", ""),
                            "title": item.get("title", ""),
                            "content": item.get("content", ""),
                            "source_name": f"Perplexity Search ({lang.upper()})",
                            "published_at": published_at or datetime.utcnow(),
                        }

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                        if article_data["url"] and article_data["title"]:
                            articles.append(article_data)

                            logger.info(
                                "perplexity_article_fetched",
                                lang=lang,
                                title=article_data["title"][:50]
                            )

                    except Exception as e:
                        logger.error(
                            "perplexity_article_parse_error",
                            error=str(e),
                            item=str(item)[:200]
                        )
                        continue

            except json.JSONDecodeError as e:
                logger.error(
                    "perplexity_json_parse_error",
                    error=str(e),
                    response=response[:500]
                )

        except Exception as e:
            logger.error(
                "perplexity_fetch_error",
                lang=lang,
                error=str(e)
            )

        logger.info(
            "perplexity_fetch_complete",
            lang=lang,
            articles_count=len(articles)
        )

        return articles

    async def save_articles(self, articles: List[Dict[str, Any]]) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç—å–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.

        Args:
            articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        """
        saved_count = 0

        for article_data in articles:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞—Ç—å—è —Å —Ç–∞–∫–∏–º URL
                result = await self.db.execute(
                    select(RawArticle).where(RawArticle.url == article_data["url"])
                )
                existing = result.scalar_one_or_none()

                if existing:
                    logger.debug(
                        "article_exists",
                        url=article_data["url"]
                    )
                    continue

                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç–∞—Ç—å—é
                article = RawArticle(**article_data)
                self.db.add(article)
                saved_count += 1

                logger.info(
                    "article_saved",
                    url=article_data["url"],
                    title=article_data["title"][:50]
                )

            except Exception as e:
                logger.error(
                    "article_save_error",
                    error=str(e),
                    url=article_data.get("url", "Unknown")
                )
                continue

        await self.db.commit()

        logger.info("articles_save_complete", saved_count=saved_count)

        return saved_count

    async def fetch_all_sources(self) -> Dict[str, int]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç–∞—Ç–µ–π –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        """
        from app.modules.settings_manager import is_source_enabled

        stats = {}

        # Google News RSS (—Ä—É—Å—Å–∫–∏–π)
        if settings.fetcher_enabled and await is_source_enabled("google_news_ru", self.db):
            logger.info("fetching_source", source="google_news_ru", enabled=True)
            articles_ru = await self.fetch_google_news_rss("ru")
            saved_ru = await self.save_articles(articles_ru)
            stats["Google News RU"] = saved_ru
        else:
            logger.info("source_disabled", source="google_news_ru")

        # Google News RSS (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        if settings.fetcher_enabled and await is_source_enabled("google_news_en", self.db):
            logger.info("fetching_source", source="google_news_en", enabled=True)
            articles_en = await self.fetch_google_news_rss("en")
            saved_en = await self.save_articles(articles_en)
            stats["Google News EN"] = saved_en
        else:
            logger.info("source_disabled", source="google_news_en")

        # Perplexity Real-Time Search (—Ä—É—Å—Å–∫–∏–π)
        if settings.perplexity_search_enabled and await is_source_enabled("perplexity_ru", self.db):
            logger.info("fetching_source", source="perplexity_ru", enabled=True)
            perplexity_articles_ru = await self.fetch_perplexity_news("ru")
            saved_perplexity_ru = await self.save_articles(perplexity_articles_ru)
            stats["Perplexity Search RU"] = saved_perplexity_ru
        else:
            logger.info("source_disabled", source="perplexity_ru")

        # Perplexity Real-Time Search (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
        if settings.perplexity_search_enabled and await is_source_enabled("perplexity_en", self.db):
            logger.info("fetching_source", source="perplexity_en", enabled=True)
            perplexity_articles_en = await self.fetch_perplexity_news("en")
            saved_perplexity_en = await self.save_articles(perplexity_articles_en)
            stats["Perplexity Search EN"] = saved_perplexity_en
        else:
            logger.info("source_disabled", source="perplexity_en")

        # üÜï Telegram Channels (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
        if (settings.telegram_fetch_enabled and
            settings.telegram_api_id and
            settings.telegram_api_hash and
            await is_source_enabled("telegram_channels", self.db)):
            logger.info("fetching_source", source="telegram_channels", enabled=True)

            from app.modules.telegram_fetcher import fetch_telegram_news

            telegram_stats, telegram_articles = await fetch_telegram_news()
            saved_telegram = await self.save_articles(telegram_articles)

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞–∂–¥–æ–º—É –∫–∞–Ω–∞–ª—É
            for channel_name, count in telegram_stats.items():
                # count - —ç—Ç–æ —Å–∫–æ–ª—å–∫–æ –±—ã–ª–æ —Å–æ–±—Ä–∞–Ω–æ –î–û —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
                # –ù–æ –Ω–∞–º –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å —Å–∫–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏–ª–æ—Å—å
                # –ü–æ—ç—Ç–æ–º—É –ø—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—â–∏–π —Å—á–µ—Ç—á–∏–∫
                pass

            # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ Telegram
            stats["Telegram Channels"] = saved_telegram

            # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞–Ω–∞–ª–∞–º (–¥–ª—è –ª–æ–≥–æ–≤)
            logger.info(
                "telegram_detailed_stats",
                channels_stats=telegram_stats,
                total_saved=saved_telegram
            )
        else:
            logger.info("source_disabled", source="telegram_channels")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –ë–î
        result = await self.db.execute(
            select(Source).where(Source.enabled == True, Source.type == "rss")
        )
        sources = result.scalars().all()

        for source in sources:
            try:
                articles = await self.fetch_rss_feed(source)
                saved = await self.save_articles(articles)
                stats[source.name] = saved

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                source.last_fetch = datetime.utcnow()
                source.fetch_errors = 0

            except Exception as e:
                logger.error(
                    "source_fetch_failed",
                    source_name=source.name,
                    error=str(e)
                )
                source.fetch_errors += 1

        await self.db.commit()

        # –õ–æ–≥–∏—Ä—É–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_articles = sum(stats.values())
        await log_to_db(
            "INFO",
            f"Fetch completed: {total_articles} articles from {len(stats)} sources",
            {"stats": stats},
            session=self.db  # –ü–µ—Ä–µ–¥–∞—ë–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–µ—Å—Å–∏—é
        )

        logger.info(
            "fetch_all_complete",
            total_articles=total_articles,
            sources_count=len(stats),
            stats=stats
        )

        return stats


async def fetch_news(db_session: AsyncSession) -> Dict[str, int]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.

    Args:
        db_session: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è –ë–î

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –Ω–æ–≤–æ—Å—Ç—è–º
    """
    async with NewsFetcher(db_session) as fetcher:
        return await fetcher.fetch_all_sources()
