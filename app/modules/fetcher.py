"""
News Fetcher Module
–õ–µ–≥–∞–ª—å–Ω—ã–π —Å–±–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –∏–∑ –ø—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–ò—Å—Ç–æ—á–Ω–∏–∫–∏:
1. Google News RSS (—Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
2. –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏
3. Telegram –∫–∞–Ω–∞–ª—ã (—Ç–æ–ª—å–∫–æ —á–µ—Ä–µ–∑ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —ç–∫—Å–ø–æ—Ä—Ç)
"""

import asyncio
import json
import random
from datetime import datetime
from typing import List, Optional, Dict, Any
from urllib.parse import urlencode, quote_plus

import feedparser
import httpx
from bs4 import BeautifulSoup
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession

from app.config import settings
from app.models.database import RawArticle, Source, log_to_db
import structlog

logger = structlog.get_logger()


# User-Agent —Ä–æ—Ç–∞—Ü–∏—è –¥–ª—è –ª–µ–≥–∞–ª—å–Ω–æ–≥–æ —Å–∫—Ä–∞–ø–∏–Ω–≥–∞
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
]


class NewsFetcher:
    """–°–±–æ—Ä—â–∏–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤."""

    def __init__(self, db_session: AsyncSession):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è fetcher.

        Args:
            db_session: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        """
        self.db = db_session
        self.client: Optional[httpx.AsyncClient] = None

    async def __aenter__(self):
        """Async context manager entry."""
        self.client = httpx.AsyncClient(
            timeout=settings.fetcher_request_timeout,
            follow_redirects=True,
            headers={"User-Agent": self._get_random_user_agent()}
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self.client:
            await self.client.aclose()

    def _get_random_user_agent(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ª—É—á–∞–π–Ω—ã–π User-Agent –¥–ª—è —Ä–æ—Ç–∞—Ü–∏–∏."""
        return random.choice(USER_AGENTS)

    async def _fetch_with_retry(
        self,
        url: str,
        max_retries: Optional[int] = None
    ) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç —Å retry –º–µ—Ö–∞–Ω–∏–∑–º–æ–º.

        Args:
            url: URL –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            max_retries: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫

        Returns:
            –ö–æ–Ω—Ç–µ–Ω—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        if max_retries is None:
            max_retries = settings.fetcher_max_retries

        for attempt in range(max_retries):
            try:
                # Rate limiting - 1 –∑–∞–ø—Ä–æ—Å –≤ —Å–µ–∫—É–Ω–¥—É
                if attempt > 0:
                    delay = settings.fetcher_retry_delay * (2 ** attempt)  # Exponential backoff
                    await asyncio.sleep(delay)
                else:
                    await asyncio.sleep(1)  # Base rate limit

                # –û–±–Ω–æ–≤–ª—è–µ–º User-Agent –¥–ª—è –∫–∞–∂–¥–æ–π –ø–æ–ø—ã—Ç–∫–∏
                self.client.headers["User-Agent"] = self._get_random_user_agent()

                response = await self.client.get(url)
                response.raise_for_status()

                logger.info(
                    "fetch_success",
                    url=url,
                    status_code=response.status_code,
                    attempt=attempt + 1
                )

                return response.text

            except httpx.HTTPError as e:
                logger.warning(
                    "fetch_error",
                    url=url,
                    error=str(e),
                    attempt=attempt + 1,
                    max_retries=max_retries
                )

                if attempt == max_retries - 1:
                    logger.error(
                        "fetch_failed",
                        url=url,
                        error=str(e),
                        total_attempts=max_retries
                    )
                    await log_to_db(
                        "ERROR",
                        f"Failed to fetch URL after {max_retries} attempts",
                        {"url": url, "error": str(e)},
                        session=self.db  # –ü–µ—Ä–µ–¥–∞—ë–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–µ—Å—Å–∏—é
                    )
                    return None

        return None

    def _build_google_news_rss_url(
        self,
        query: str,
        lang: str = "ru",
        region: str = "RU"
    ) -> str:
        """
        –ü–æ—Å—Ç—Ä–æ–∏—Ç—å URL –¥–ª—è Google News RSS.

        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            lang: –Ø–∑—ã–∫ (ru, en)
            region: –†–µ–≥–∏–æ–Ω (RU, US)

        Returns:
            URL –¥–ª—è RSS feed
        """
        params = {
            "q": query,
            "hl": lang,
            "gl": region,
            "ceid": f"{region}:{lang}"
        }
        return f"{settings.google_news_rss_url}?{urlencode(params, quote_via=quote_plus)}"

    async def fetch_google_news_rss(self, lang: str = "ru") -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Google News RSS.

        Args:
            lang: –Ø–∑—ã–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (ru –∏–ª–∏ en)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∏ —Ä–µ–≥–∏–æ–Ω –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        if lang == "ru":
            query = settings.google_news_query_ru
            region = settings.google_news_region
        else:
            query = settings.google_news_query_en
            region = "US"

        rss_url = self._build_google_news_rss_url(query, lang, region)

        logger.info("fetching_google_news", lang=lang, url=rss_url)

        # –ü–æ–ª—É—á–∞–µ–º RSS feed
        content = await self._fetch_with_retry(rss_url)
        if not content:
            return articles

        # –ü–∞—Ä—Å–∏–º RSS
        feed = feedparser.parse(content)

        for entry in feed.entries[:settings.fetcher_max_articles_per_source]:
            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ RSS entry
                article_data = {
                    "url": entry.link,
                    "title": entry.title,
                    "content": entry.get("summary", ""),
                    "source_name": f"Google News RSS ({lang.upper()})",
                    "published_at": self._parse_date(entry.get("published")),
                }

                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏
                full_content = await self._fetch_article_content(entry.link)
                if full_content:
                    article_data["content"] = full_content

                articles.append(article_data)

                logger.info(
                    "article_fetched",
                    source="google_news",
                    lang=lang,
                    title=article_data["title"][:50]
                )

            except Exception as e:
                logger.error(
                    "article_parse_error",
                    error=str(e),
                    entry_title=entry.get("title", "Unknown")
                )
                continue

        logger.info(
            "google_news_fetch_complete",
            lang=lang,
            articles_count=len(articles)
        )

        return articles

    async def _fetch_article_content(self, url: str) -> Optional[str]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.

        Args:
            url: URL —Å—Ç–∞—Ç—å–∏

        Returns:
            –¢–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None
        """
        try:
            content = await self._fetch_with_retry(url)
            if not content:
                return None

            # –ü–∞—Ä—Å–∏–º HTML —Å –ø–æ–º–æ—â—å—é BeautifulSoup
            soup = BeautifulSoup(content, "html.parser")

            # –£–¥–∞–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç—ã –∏ —Å—Ç–∏–ª–∏
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()

            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            # –ò—â–µ–º –ø–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–º —Ç–µ–≥–∞–º –¥–ª—è —Å—Ç–∞—Ç–µ–π
            article_tags = [
                soup.find("article"),
                soup.find("div", class_=lambda x: x and "content" in x.lower()),
                soup.find("div", class_=lambda x: x and "article" in x.lower()),
                soup.find("main"),
            ]

            for tag in article_tags:
                if tag:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç
                    text = tag.get_text(separator="\n", strip=True)
                    # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ –ø–µ—Ä–µ–Ω–æ—Å–æ–≤
                    text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
                    if len(text) > 200:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –¥–ª—è –≤–∞–ª–∏–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        return text[:5000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ–≥–∏, –±–µ—Ä–µ–º –≤–µ—Å—å body
            body = soup.find("body")
            if body:
                text = body.get_text(separator="\n", strip=True)
                text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
                return text[:5000]

        except Exception as e:
            logger.warning(
                "content_fetch_error",
                url=url,
                error=str(e)
            )

        return None

    async def fetch_rss_feed(self, source: Source) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∞.

        Args:
            source: –û–±—ä–µ–∫—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –∏–∑ –ë–î

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        logger.info("fetching_rss", source_name=source.name, url=source.url)

        content = await self._fetch_with_retry(source.url)
        if not content:
            return articles

        feed = feedparser.parse(content)

        for entry in feed.entries[:settings.fetcher_max_articles_per_source]:
            try:
                article_data = {
                    "url": entry.link,
                    "title": entry.title,
                    "content": entry.get("summary", ""),
                    "source_name": source.name,
                    "published_at": self._parse_date(entry.get("published")),
                }

                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –ø–æ–ª–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
                full_content = await self._fetch_article_content(entry.link)
                if full_content:
                    article_data["content"] = full_content

                articles.append(article_data)

            except Exception as e:
                logger.error(
                    "rss_parse_error",
                    source=source.name,
                    error=str(e)
                )
                continue

        logger.info(
            "rss_fetch_complete",
            source_name=source.name,
            articles_count=len(articles)
        )

        return articles

    def _parse_date(self, date_str: Optional[str]) -> Optional[datetime]:
        """
        –ü–∞—Ä—Å–∏—Ç—å –¥–∞—Ç—É –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤.

        Args:
            date_str: –°—Ç—Ä–æ–∫–∞ —Å –¥–∞—Ç–æ–π

        Returns:
            datetime –æ–±—ä–µ–∫—Ç –∏–ª–∏ None (–±–µ–∑ timezone)
        """
        if not date_str:
            return None

        try:
            # feedparser –æ–±—ã—á–Ω–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç parsed –≤—Ä–µ–º—è
            from email.utils import parsedate_to_datetime
            dt = parsedate_to_datetime(date_str)
            # –£–±–∏—Ä–∞–µ–º timezone –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
            return dt.replace(tzinfo=None) if dt else None
        except Exception:
            try:
                # Fallback –Ω–∞ ISO —Ñ–æ—Ä–º–∞—Ç
                from dateutil import parser
                dt = parser.parse(date_str)
                # –£–±–∏—Ä–∞–µ–º timezone –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –ë–î
                return dt.replace(tzinfo=None) if dt else None
            except Exception:
                logger.warning("date_parse_error", date_str=date_str)
                return None

    async def fetch_perplexity_news(self, lang: str = "ru") -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Perplexity AI real-time search.

        Args:
            lang: –Ø–∑—ã–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (ru –∏–ª–∏ en)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —è–∑—ã–∫–∞
        if lang == "ru":
            query = settings.google_news_query_ru.replace(" AND ", " ")
            search_prompt = f"""–ù–∞–π–¥–∏ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 24 —á–∞—Å–∞) –ø–æ –∑–∞–ø—Ä–æ—Å—É: {query}

–í–µ—Ä–Ω–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞, –≥–¥–µ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç:
- title: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–æ–≤–æ—Å—Ç–∏
- content: –∫—Ä–∞—Ç–∫–æ–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- url: —Å—Å—ã–ª–∫–∞ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫
- source_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
- published_at: –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ ISO 8601

–ò—â–∏ —Ç–æ–ª—å–∫–æ –∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏. –í–µ—Ä–Ω–∏ –º–∞–∫—Å–∏–º—É–º 10 –Ω–æ–≤–æ—Å—Ç–µ–π."""
        else:
            query = settings.google_news_query_en.replace(" AND ", " ")
            search_prompt = f"""Find latest news (from last 24 hours) for query: {query}

Return results as JSON array where each element contains:
- title: news headline
- content: brief summary (2-3 sentences)
- url: source link
- source_name: source name
- published_at: publication date in ISO 8601 format

Search only for recent news. Return maximum 10 articles."""

        logger.info("fetching_perplexity_news", lang=lang)

        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º LLM provider –¥–ª—è Perplexity
            from app.modules.llm_provider import get_llm_provider

            llm = get_llm_provider("perplexity")

            # –î–µ–ª–∞–µ–º –∑–∞–ø—Ä–æ—Å –∫ Perplexity —Å real-time search
            response = await llm.generate_completion(
                messages=[
                    {"role": "system", "content": "You are a news aggregator assistant. Always return valid JSON."},
                    {"role": "user", "content": search_prompt}
                ],
                max_tokens=3000,
                temperature=0.3
            )

            # –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
            import json
            import re

            # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–±–µ—Ä–Ω—É—Ç –≤ markdown)
            json_match = re.search(r'```json\s*(.*?)\s*```', response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å –≤–µ—Å—å –æ—Ç–≤–µ—Ç –∫–∞–∫ JSON
                json_str = response.strip()

            try:
                news_data = json.loads(json_str)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ —Å–ø–∏—Å–æ–∫
                if not isinstance(news_data, list):
                    logger.warning("perplexity_response_not_list", response=response[:200])
                    return articles

                for item in news_data:
                    try:
                        # –ü–∞—Ä—Å–∏–º –¥–∞—Ç—É –µ—Å–ª–∏ –µ—Å—Ç—å
                        published_at = None
                        if "published_at" in item and item["published_at"]:
                            published_at = self._parse_date(item["published_at"])

                        # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é
                        article_data = {
                            "url": item.get("url", ""),
                            "title": item.get("title", ""),
                            "content": item.get("content", ""),
                            "source_name": f"Perplexity Search ({lang.upper()})",
                            "published_at": published_at or datetime.utcnow(),
                        }

                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
                        if article_data["url"] and article_data["title"]:
                            articles.append(article_data)

                            logger.info(
                                "perplexity_article_fetched",
                                lang=lang,
                                title=article_data["title"][:50]
                            )

                    except Exception as e:
                        logger.error(
                            "perplexity_article_parse_error",
                            error=str(e),
                            item=str(item)[:200]
                        )
                        continue

            except json.JSONDecodeError as e:
                logger.error(
                    "perplexity_json_parse_error",
                    error=str(e),
                    response=response[:500]
                )

        except Exception as e:
            logger.error(
                "perplexity_fetch_error",
                lang=lang,
                error=str(e)
            )

        logger.info(
            "perplexity_fetch_complete",
            lang=lang,
            articles_count=len(articles)
        )

        return articles

    async def fetch_hackernews(self) -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Hacker News API.

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        logger.info("fetching_hackernews")

        try:
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-500 –∏—Å—Ç–æ—Ä–∏–π
            top_stories_url = "https://hacker-news.firebaseio.com/v0/topstories.json"
            response = await self._fetch_with_retry(top_stories_url)

            if not response:
                return articles

            story_ids = json.loads(response)

            # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            keywords = [
                'ai', 'artificial intelligence', 'machine learning', 'ml',
                'legal tech', 'legaltech', 'law', 'lawyer', 'court',
                'automation', 'neural', 'llm', 'gpt', 'openai',
                'compliance', 'contract', 'regulation'
            ]

            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100 –∏—Å—Ç–æ—Ä–∏–π (—Ç–æ–ø —Å–∞–º—ã–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ)
            checked_count = 0
            for story_id in story_ids[:100]:
                if len(articles) >= 10:  # –õ–∏–º–∏—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    break

                try:
                    # –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª–∏ –∏—Å—Ç–æ—Ä–∏–∏
                    story_url = f"https://hacker-news.firebaseio.com/v0/item/{story_id}.json"
                    story_response = await self._fetch_with_retry(story_url)

                    if not story_response:
                        continue

                    story = json.loads(story_response)

                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ stories (–Ω–µ jobs, polls)
                    if story.get('type') != 'story':
                        continue

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ URL
                    if not story.get('url'):
                        continue

                    title = story.get('title', '')
                    text = story.get('text', '')

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                    combined_text = f"{title} {text}".lower()
                    is_relevant = any(keyword in combined_text for keyword in keywords)

                    if not is_relevant:
                        checked_count += 1
                        continue

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–∞—Ç—É
                    published_at = None
                    if 'time' in story:
                        from datetime import datetime
                        published_at = datetime.utcfromtimestamp(story['time'])

                    # –°–æ–∑–¥–∞–µ–º —Å—Ç–∞—Ç—å—é
                    article_data = {
                        "url": story['url'],
                        "title": title,
                        "content": text or f"{title}\n\nDiscussion: https://news.ycombinator.com/item?id={story_id}",
                        "source_name": "Hacker News",
                        "published_at": published_at,
                    }

                    articles.append(article_data)

                    logger.info(
                        "hackernews_article_fetched",
                        title=title[:50],
                        score=story.get('score', 0)
                    )

                    checked_count += 1

                except Exception as e:
                    logger.error(
                        "hackernews_story_parse_error",
                        story_id=story_id,
                        error=str(e)
                    )
                    continue

            logger.info(
                "hackernews_fetch_complete",
                articles_count=len(articles),
                checked_count=checked_count
            )

        except Exception as e:
            logger.error(
                "hackernews_fetch_error",
                error=str(e)
            )

        return articles

    async def fetch_reddit(self, subreddit: str = "MachineLearning") -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Reddit (–±–µ–∑ OAuth, —á–µ—Ä–µ–∑ JSON API).

        Args:
            subreddit: –ù–∞–∑–≤–∞–Ω–∏–µ subreddit

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        logger.info("fetching_reddit", subreddit=subreddit)

        try:
            # Reddit JSON API (–Ω–µ —Ç—Ä–µ–±—É–µ—Ç OAuth –¥–ª—è –ø—É–±–ª–∏—á–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤)
            reddit_url = f"https://www.reddit.com/r/{subreddit}/hot.json?limit=25"

            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π User-Agent –¥–ª—è Reddit API
            old_user_agent = self.client.headers.get("User-Agent")
            self.client.headers["User-Agent"] = "LegalTechNewsBot/1.0 (AI News Aggregator)"

            response = await self._fetch_with_retry(reddit_url)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ç–∞—Ä—ã–π User-Agent
            if old_user_agent:
                self.client.headers["User-Agent"] = old_user_agent

            if not response:
                return articles

            data = json.loads(response)

            # –ü–∞—Ä—Å–∏–º –ø–æ—Å—Ç—ã
            for post in data.get('data', {}).get('children', []):
                try:
                    post_data = post.get('data', {})

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º stickied –ø–æ—Å—Ç—ã
                    if post_data.get('stickied'):
                        continue

                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —É–¥–∞–ª–µ–Ω–Ω—ã–µ
                    if post_data.get('removed_by_category'):
                        continue

                    title = post_data.get('title', '')
                    selftext = post_data.get('selftext', '')
                    url = post_data.get('url', '')
                    permalink = f"https://www.reddit.com{post_data.get('permalink', '')}"

                    # –ï—Å–ª–∏ —ç—Ç–æ self post (—Ç–µ–∫—Å—Ç–æ–≤—ã–π), –∏—Å–ø–æ–ª—å–∑—É–µ–º permalink
                    if post_data.get('is_self'):
                        url = permalink

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    content = selftext[:1000] if selftext else title

                    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                    score = post_data.get('score', 0)
                    num_comments = post_data.get('num_comments', 0)
                    content += f"\n\nüëç {score} upvotes | üí¨ {num_comments} comments"

                    # –î–∞—Ç–∞
                    published_at = None
                    if 'created_utc' in post_data:
                        published_at = datetime.utcfromtimestamp(post_data['created_utc'])

                    article_data = {
                        "url": url,
                        "title": title,
                        "content": content,
                        "source_name": f"Reddit r/{subreddit}",
                        "published_at": published_at,
                    }

                    articles.append(article_data)

                    logger.info(
                        "reddit_post_fetched",
                        subreddit=subreddit,
                        title=title[:50],
                        score=score
                    )

                    # –õ–∏–º–∏—Ç
                    if len(articles) >= 10:
                        break

                except Exception as e:
                    logger.error(
                        "reddit_post_parse_error",
                        subreddit=subreddit,
                        error=str(e)
                    )
                    continue

            logger.info(
                "reddit_fetch_complete",
                subreddit=subreddit,
                articles_count=len(articles)
            )

        except Exception as e:
            logger.error(
                "reddit_fetch_error",
                subreddit=subreddit,
                error=str(e)
            )

        return articles

    async def fetch_arxiv(self, category: str = "cs.AI") -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ ArXiv API.

        Args:
            category: –ö–∞—Ç–µ–≥–æ—Ä–∏—è (cs.AI, cs.LG, cs.CL)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        logger.info("fetching_arxiv", category=category)

        try:
            # ArXiv API query
            # –ò—â–µ–º —Å—Ç–∞—Ç—å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ –¥–∞—Ç–µ
            arxiv_url = (
                f"http://export.arxiv.org/api/query?"
                f"search_query=cat:{category}&"
                f"sortBy=submittedDate&"
                f"sortOrder=descending&"
                f"max_results=20"
            )

            response = await self._fetch_with_retry(arxiv_url)

            if not response:
                return articles

            # –ü–∞—Ä—Å–∏–º XML –æ—Ç–≤–µ—Ç
            from xml.etree import ElementTree as ET

            root = ET.fromstring(response)

            # Namespace –¥–ª—è ArXiv
            ns = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }

            # –ü–∞—Ä—Å–∏–º entries
            for entry in root.findall('atom:entry', ns):
                try:
                    title_elem = entry.find('atom:title', ns)
                    summary_elem = entry.find('atom:summary', ns)
                    link_elem = entry.find('atom:id', ns)
                    published_elem = entry.find('atom:published', ns)

                    if not all([title_elem, summary_elem, link_elem]):
                        continue

                    title = title_elem.text.strip().replace('\n', ' ')
                    summary = summary_elem.text.strip().replace('\n', ' ')[:500]
                    url = link_elem.text.strip()

                    # –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    published_at = None
                    if published_elem is not None:
                        published_at = self._parse_date(published_elem.text)

                    # –ê–≤—Ç–æ—Ä—ã
                    authors = []
                    for author in entry.findall('atom:author', ns):
                        name_elem = author.find('atom:name', ns)
                        if name_elem is not None:
                            authors.append(name_elem.text)

                    authors_str = ', '.join(authors[:3])  # –ü–µ—Ä–≤—ã–µ 3 –∞–≤—Ç–æ—Ä–∞
                    if len(authors) > 3:
                        authors_str += ' et al.'

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    content = f"{summary}\n\nAuthors: {authors_str}"

                    article_data = {
                        "url": url,
                        "title": title,
                        "content": content,
                        "source_name": f"ArXiv {category}",
                        "published_at": published_at,
                    }

                    articles.append(article_data)

                    logger.info(
                        "arxiv_article_fetched",
                        category=category,
                        title=title[:50]
                    )

                    # –õ–∏–º–∏—Ç
                    if len(articles) >= 5:  # –ú–µ–Ω—å—à–µ –Ω–∞—É—á–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π, –æ–Ω–∏ –¥–ª–∏–Ω–Ω–µ–µ
                        break

                except Exception as e:
                    logger.error(
                        "arxiv_entry_parse_error",
                        category=category,
                        error=str(e)
                    )
                    continue

            logger.info(
                "arxiv_fetch_complete",
                category=category,
                articles_count=len(articles)
            )

        except Exception as e:
            logger.error(
                "arxiv_fetch_error",
                category=category,
                error=str(e)
            )

        return articles

    async def fetch_medium_rss(self, tag: str = "artificial-intelligence") -> List[Dict[str, Any]]:
        """
        –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç—å–∏ –∏–∑ Medium –ø–æ —Ç–µ–≥—É —á–µ—Ä–µ–∑ RSS.

        Args:
            tag: –¢–µ–≥ –Ω–∞ Medium (artificial-intelligence, machine-learning, legaltech)

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏
        """
        articles = []

        logger.info("fetching_medium", tag=tag)

        try:
            # Medium RSS feed –¥–ª—è —Ç–µ–≥–∞
            medium_url = f"https://medium.com/feed/tag/{tag}"

            response = await self._fetch_with_retry(medium_url)

            if not response:
                return articles

            # –ü–∞—Ä—Å–∏–º RSS
            feed = feedparser.parse(response)

            for entry in feed.entries[:10]:  # –õ–∏–º–∏—Ç 10 —Å—Ç–∞—Ç–µ–π
                try:
                    title = entry.title
                    summary = entry.get('summary', '')

                    # –£–±–∏—Ä–∞–µ–º HTML —Ç–µ–≥–∏ –∏–∑ summary
                    from bs4 import BeautifulSoup
                    soup = BeautifulSoup(summary, 'html.parser')
                    clean_summary = soup.get_text()[:500]

                    url = entry.link

                    # –î–∞—Ç–∞
                    published_at = self._parse_date(entry.get('published'))

                    # –ê–≤—Ç–æ—Ä
                    author = entry.get('author', 'Unknown')

                    # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
                    content = f"{clean_summary}\n\nAuthor: {author}"

                    article_data = {
                        "url": url,
                        "title": title,
                        "content": content,
                        "source_name": f"Medium ({tag})",
                        "published_at": published_at,
                    }

                    articles.append(article_data)

                    logger.info(
                        "medium_article_fetched",
                        tag=tag,
                        title=title[:50]
                    )

                except Exception as e:
                    logger.error(
                        "medium_entry_parse_error",
                        tag=tag,
                        error=str(e)
                    )
                    continue

            logger.info(
                "medium_fetch_complete",
                tag=tag,
                articles_count=len(articles)
            )

        except Exception as e:
            logger.error(
                "medium_fetch_error",
                tag=tag,
                error=str(e)
            )

        return articles

    async def save_articles(self, articles: List[Dict[str, Any]]) -> int:
        """
        –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—Ç–∞—Ç—å–∏ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.

        Args:
            articles: –°–ø–∏—Å–æ–∫ —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è

        Returns:
            –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π
        """
        saved_count = 0

        for article_data in articles:
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç–∞—Ç—å—è —Å —Ç–∞–∫–∏–º URL
                result = await self.db.execute(
                    select(RawArticle).where(RawArticle.url == article_data["url"])
                )
                existing = result.scalar_one_or_none()

                if existing:
                    logger.debug(
                        "article_exists",
                        url=article_data["url"]
                    )
                    continue

                # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å—Ç–∞—Ç—å—é
                article = RawArticle(**article_data)
                self.db.add(article)
                saved_count += 1

                logger.info(
                    "article_saved",
                    url=article_data["url"],
                    title=article_data["title"][:50]
                )

            except Exception as e:
                logger.error(
                    "article_save_error",
                    error=str(e),
                    url=article_data.get("url", "Unknown")
                )
                continue

        await self.db.commit()

        logger.info("articles_save_complete", saved_count=saved_count)

        return saved_count

    async def fetch_all_sources(self) -> Dict[str, int]:
        """
        –ü–æ–ª—É—á–∏—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –≤—Å–µ—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å—Ç–∞—Ç–µ–π –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º
        """
        stats = {}

        # Google News RSS (—Ä—É—Å—Å–∫–∏–π)
        if settings.fetcher_enabled:
            articles_ru = await self.fetch_google_news_rss("ru")
            saved_ru = await self.save_articles(articles_ru)
            stats["Google News RU"] = saved_ru

            # Google News RSS (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π)
            articles_en = await self.fetch_google_news_rss("en")
            saved_en = await self.save_articles(articles_en)
            stats["Google News EN"] = saved_en

            # Perplexity Real-Time Search (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω)
            if settings.perplexity_search_enabled:
                # –†—É—Å—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Perplexity
                perplexity_articles_ru = await self.fetch_perplexity_news("ru")
                saved_perplexity_ru = await self.save_articles(perplexity_articles_ru)
                stats["Perplexity Search RU"] = saved_perplexity_ru

                # –ê–Ω–≥–ª–∏–π—Å–∫–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ Perplexity
                perplexity_articles_en = await self.fetch_perplexity_news("en")
                saved_perplexity_en = await self.save_articles(perplexity_articles_en)
                stats["Perplexity Search EN"] = saved_perplexity_en

            # Hacker News
            if settings.hackernews_enabled:
                hn_articles = await self.fetch_hackernews()
                saved_hn = await self.save_articles(hn_articles)
                stats["Hacker News"] = saved_hn

            # Reddit - –Ω–µ—Å–∫–æ–ª—å–∫–æ subreddits
            if settings.reddit_enabled:
                for subreddit in settings.reddit_subreddits_list:
                    reddit_articles = await self.fetch_reddit(subreddit)
                    saved_reddit = await self.save_articles(reddit_articles)
                    stats[f"Reddit r/{subreddit}"] = saved_reddit

            # ArXiv - –Ω–∞—É—á–Ω—ã–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            if settings.arxiv_enabled:
                for category in settings.arxiv_categories_list:
                    arxiv_articles = await self.fetch_arxiv(category)
                    saved_arxiv = await self.save_articles(arxiv_articles)
                    stats[f"ArXiv {category}"] = saved_arxiv

            # Medium
            if settings.medium_enabled:
                for tag in settings.medium_tags_list:
                    medium_articles = await self.fetch_medium_rss(tag)
                    saved_medium = await self.save_articles(medium_articles)
                    stats[f"Medium {tag}"] = saved_medium

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ RSS –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏–∑ –ë–î
        result = await self.db.execute(
            select(Source).where(Source.enabled == True, Source.type == "rss")
        )
        sources = result.scalars().all()

        for source in sources:
            try:
                articles = await self.fetch_rss_feed(source)
                saved = await self.save_articles(articles)
                stats[source.name] = saved

                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞
                source.last_fetch = datetime.utcnow()
                source.fetch_errors = 0

            except Exception as e:
                logger.error(
                    "source_fetch_failed",
                    source_name=source.name,
                    error=str(e)
                )
                source.fetch_errors += 1

        await self.db.commit()

        # –õ–æ–≥–∏—Ä—É–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        total_articles = sum(stats.values())
        await log_to_db(
            "INFO",
            f"Fetch completed: {total_articles} articles from {len(stats)} sources",
            {"stats": stats},
            session=self.db  # –ü–µ—Ä–µ–¥–∞—ë–º —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Å–µ—Å—Å–∏—é
        )

        logger.info(
            "fetch_all_complete",
            total_articles=total_articles,
            sources_count=len(stats),
            stats=stats
        )

        return stats


async def fetch_news(db_session: AsyncSession) -> Dict[str, int]:
    """
    –£–¥–æ–±–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π.

    Args:
        db_session: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Å–µ—Å—Å–∏—è –ë–î

    Returns:
        –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–æ–±—Ä–∞–Ω–Ω—ã–º –Ω–æ–≤–æ—Å—Ç—è–º
    """
    async with NewsFetcher(db_session) as fetcher:
        return await fetcher.fetch_all_sources()
